{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a670d824",
   "metadata": {},
   "source": [
    "\n",
    "# Job Shop Scheduling Problem \n",
    "Here We want to implement job shop scheduling simulator.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b4a3ce",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0fcbef8",
   "metadata": {},
   "source": [
    "## State Representaion:\n",
    "\n",
    "-  In previous RL-based scheduling methods, state feature was defind as a some indicatorus of the production status, i.e. number of machines/jobs/operations in shop floor, the remaining processing time of uncompleted jobs, the current workload/queue length of each machine and so on. However, the problem of this approach is that in real world the number of jobs/machine/operations are large and can vary in a wide range, and taking these indicators as state decrease the generalizability of the RL agent. Since it can only perform well under the same same state size.\n",
    "- To seprate the state representaion from the the direct indicator mentiond earlier, we use seven elaborately state feature with each value in the range of [0, 1].\n",
    "\n",
    "At each rescheduling point *t*, the environment state is represented by the following features:\n",
    "\n",
    "1) Average machine utilization U_ave(t):\n",
    "   U_ave(t) = (sum_{k=1..m} U_k(t)) / m\n",
    "\n",
    "2) Std. dev. of machine utilization U_std(t):\n",
    "   U_std(t) = sqrt( (sum_{k=1..m} (U_k(t) - U_ave(t))^2) / m )\n",
    "\n",
    "3) Average operation completion rate CRO_ave(t):\n",
    "   CRO_ave(t) = (sum_{i=1..n} OP_i(t)) / (sum_{i=1..n} n_i)\n",
    "\n",
    "4) Average job completion rate CRJ_ave(t):\n",
    "   CRJ_ave(t) = (sum_{i=1..n} CRJ_i(t)) / n\n",
    "\n",
    "5) Std. dev. of job completion rate CRJ_std(t):\n",
    "   CRJ_std(t) = sqrt( (sum_{i=1..n} (CRJ_i(t) - CRJ_ave(t))^2) / n )\n",
    "\n",
    "6) Estimated tardiness rate Tard_e(t):\n",
    "   Tard_e(t) = N_tard / N_left  (per Algorithm 2 logic)\n",
    "\n",
    "7) Actual tardiness rate Tard_a(t):\n",
    "   Tard_a(t) = (# actual tardy operations) / (# uncompleted operations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db8cfe0",
   "metadata": {},
   "source": [
    "## Action Space:\n",
    "\n",
    "Before going explaning action space for the RL agent we need to explain what are the decision that need to be taken for succesful and efficient job shop scheduling.\n",
    "There are two main categories of decision that need to be taken, **sequencing** and **machine assignment (routing)**.\n",
    "\n",
    "- Sequencing: Determines the order in which jobs are processed on each machine.\n",
    "- Machine assignment: Determines which machine will execute a specific operation when multiple machines are capable of performing it.\n",
    "\n",
    "\n",
    "Traditionaly some rules have been used for job shop scheduling problem, but no specific rule has found to perform well across all shop configuration. Here the goal of reinforcemnet learning agent is to select between six different comosite of sequencing and machine assignment rule. In this way the reinforcement learning can learn to dispatch each rules based on the status of the system. \n",
    "\n",
    "List of the rules: \n",
    "| **Rule** | **Description** | **Formula / Logic** |\n",
    "|-----------|-----------------|----------------------|\n",
    "| **LWKRSPT** | Least Work Remaining, tie-break by Shortest Processing Time | Lexicographic rule: minimize **LWKR**, then **PT** |\n",
    "| **LWKRMOD** | Least Work Remaining, tie-break by Modified Operation Due Date | Lexicographic rule: minimize **LWKR**, then **MOD** |\n",
    "| **PTWINQ** | Processing Time plus Work In Next Queue | **Priority = PT + WINQ** |\n",
    "| **PTWINQS** | Processing Time plus Work In Next Queue plus Slack | **Priority = PT + WINQ + Slack** |\n",
    "\n",
    "| **DPTLWKRS** | Double Processing Time plus Least Work Remaining plus Slack | **Priority = 2×PT + LWKR + Slack** |\n",
    "| **DPTWINQNPT** | Double Processing Time plus Work In Next Queue plus Next Processing Time | **Priority = 2×PT + WINQ + NPT** |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42706441",
   "metadata": {},
   "source": [
    "### Rule 1: **LWKRSPT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "393dce07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a placeholder for the project code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd68c68",
   "metadata": {},
   "source": [
    "### Rule 2: **LWKRMOD**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ccf29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a placeholder for the project code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274a415f",
   "metadata": {},
   "source": [
    "### Rule 3: **PTWINQ**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b7492e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a placeholder for the project code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5f84c6",
   "metadata": {},
   "source": [
    "### Rule 4: **PTWINQS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751ee8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a placeholder for the project code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281a4f60",
   "metadata": {},
   "source": [
    "### Rule 5: **DPTLWKRS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26342cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a placeholder for the project code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7751d193",
   "metadata": {},
   "source": [
    "### Rule 6: **DPTWINQNPT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e866b2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a placeholder for the project code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09d3e3e",
   "metadata": {},
   "source": [
    "## Reward definition:\n",
    "\n",
    "The goal of the reward function is to lower these values.\n",
    "1. Lower **actual tardiness**\n",
    "2. Lower **estimated tardiness**\n",
    "3. Higher **machine utilization**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
